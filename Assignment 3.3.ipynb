{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Graded Programming Assignment\n\nIn this assignment, you will implement re-use the unsupervised anomaly detection algorithm but turn it into a simpler feed forward neural network for supervised classification.\n\nYou are training the neural network from healthy and broken samples and at later stage hook it up to a message queue for real-time anomaly detection.\n\nWe've provided a skeleton for you containing all the necessary code but left out some important parts indicated with ### your code here ###\n\nAfter you\u2019ve completed the implementation please submit it to the autograder\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!pip install tensorflow==2.2.0rc0"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import tensorflow as tf\nif not tf.__version__ == '2.2.0-rc0':\n    print(tf.__version__)\n    raise ValueError('please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)')"}, {"cell_type": "markdown", "metadata": {}, "source": "Now we import all the dependencies "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import numpy as np\nfrom numpy import concatenate\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nimport sklearn\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Activation\nimport pickle\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport sys\nfrom queue import Queue\nimport pandas as pd\nimport json\n%matplotlib inline"}, {"cell_type": "markdown", "metadata": {}, "source": "We grab the files necessary for taining. Those are sampled from the lorenz attractor model implemented in NodeRED. Those are two serialized pickle numpy arrays. In case you are interested in how these data has been generated please have a look at the following tutorial. https://developer.ibm.com/tutorials/iot-deep-learning-anomaly-detection-2/"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!rm watsoniotp.*\n!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/lorenzattractor/watsoniotp.healthy.phase_aligned.pickle\n!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/lorenzattractor/watsoniotp.broken.phase_aligned.pickle\n!mv watsoniotp.healthy.phase_aligned.pickle watsoniotp.healthy.pickle\n!mv watsoniotp.broken.phase_aligned.pickle watsoniotp.broken.pickle"}, {"cell_type": "markdown", "metadata": {}, "source": "De-serialize the numpy array containing the training data"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "data_healthy = pickle.load(open('watsoniotp.healthy.pickle', 'rb'), encoding='latin1')\ndata_broken = pickle.load(open('watsoniotp.broken.pickle', 'rb'), encoding='latin1')"}, {"cell_type": "markdown", "metadata": {}, "source": "Reshape to three columns and 3000 rows. In other words three vibration sensor axes and 3000 samples"}, {"cell_type": "markdown", "metadata": {}, "source": "Since this data is sampled from the Lorenz Attractor Model, let's plot it with a phase lot to get the typical 2-eyed plot. First for the healthy data"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "fig = plt.figure()\nax = fig.gca(projection='3d')\n\nax.plot(data_healthy[:,0], data_healthy[:,1], data_healthy[:,2],lw=0.5)\nax.set_xlabel(\"X Axis\")\nax.set_ylabel(\"Y Axis\")\nax.set_zlabel(\"Z Axis\")\nax.set_title(\"Lorenz Attractor\")"}, {"cell_type": "markdown", "metadata": {}, "source": "Then for the broken one"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "fig = plt.figure()\nax = fig.gca(projection='3d')\n\nax.plot(data_broken[:,0], data_broken[:,1], data_broken[:,2],lw=0.5)\nax.set_xlabel(\"X Axis\")\nax.set_ylabel(\"Y Axis\")\nax.set_zlabel(\"Z Axis\")\nax.set_title(\"Lorenz Attractor\")"}, {"cell_type": "markdown", "metadata": {}, "source": "In the previous examples, we fed the raw data into an LSTM. Now we want to use an ordinary feed-forward network. So we need to do some pre-processing of this time series data\n\nA widely-used method in traditional data science and signal processing is called Discrete Fourier Transformation. This algorithm transforms from the time to the frequency domain, or in other words, it returns the frequency spectrum of the signals.\n\nThe most widely used implementation of the transformation is called FFT, which stands for Fast Fourier Transformation, let\u2019s run it and see what it returns\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "data_healthy_fft = np.fft.fft(data_healthy).real\ndata_broken_fft = np.fft.fft(data_broken).real"}, {"cell_type": "markdown", "metadata": {}, "source": "Let\u2019s first have a look at the shape and contents of the arrays."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "print (data_healthy_fft.shape)\nprint (data_healthy_fft)"}, {"cell_type": "markdown", "metadata": {}, "source": "First, we notice that the shape is the same as the input data. So if we have 3000 samples, we get back 3000 spectrum values, or in other words 3000 frequency bands with the intensities.\n\nThe second thing we notice is that the data type of the array entries is not float anymore, it is complex. So those are not complex numbers, it is just a means for the algorithm the return two different frequency compositions in one go. The real part returns a sine decomposition and the imaginary part a cosine. We will ignore the cosine part in this example since it turns out that the sine part already gives us enough information to implement a good classifier.\n\nBut first let\u2019s plot the two arrays to get an idea how a healthy and broken frequency spectrum differ\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(data_healthy_fft)\nax.plot(range(0,size), data_healthy_fft[:,0].real, '-', color='blue', animated = True, linewidth=1)\nax.plot(range(0,size), data_healthy_fft[:,1].real, '-', color='red', animated = True, linewidth=1)\nax.plot(range(0,size), data_healthy_fft[:,2].real, '-', color='green', animated = True, linewidth=1)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(data_healthy_fft)\nax.plot(range(0,size), data_broken_fft[:,0].real, '-', color='blue', animated = True, linewidth=1)\nax.plot(range(0,size), data_broken_fft[:,1].real, '-', color='red', animated = True, linewidth=1)\nax.plot(range(0,size), data_broken_fft[:,2].real, '-', color='green', animated = True, linewidth=1)"}, {"cell_type": "markdown", "metadata": {}, "source": "So, what we've been doing is so called feature transformation step. We\u2019ve transformed the data set in a way that our machine learning algorithm \u2013 a deep feed forward neural network implemented as binary classifier \u2013 works better. So now let's scale the data to a 0..1"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def scaleData(data):\n    # normalize features\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    return scaler.fit_transform(data)"}, {"cell_type": "markdown", "metadata": {}, "source": "And please don\u2019t worry about the warnings. As explained before we don\u2019t need the imaginary part of the FFT"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "data_healthy_scaled = scaleData(data_healthy_fft)\ndata_broken_scaled = scaleData(data_broken_fft)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "data_healthy_scaled = data_healthy_scaled.T\ndata_broken_scaled = data_broken_scaled.T"}, {"cell_type": "markdown", "metadata": {}, "source": "Now we reshape again to have three examples (rows) and 3000 features (columns). It's important that you understand this. We have turned our initial data set which containd 3 columns (dimensions) of 3000 samples. Since we applied FFT on each column we've obtained 3000 spectrum values for each of the 3 three columns. We are now using each column with the 3000 spectrum values as one row (training example) and each of the 3000 spectrum values becomes a column (or feature) in the training data set"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "data_healthy_scaled.reshape(3, 3000)\ndata_broken_scaled.reshape(3, 3000)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Start of Assignment\n\nThe first thing we need to do is to install a little helper library for submitting the solutions to the coursera grader:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!rm -f rklib.py\n!wget https://raw.githubusercontent.com/IBM/coursera/master/rklib.py"}, {"cell_type": "markdown", "metadata": {}, "source": "Please specify you email address you are using with cousera here:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from rklib import submit, submitAll\nkey = \"4vkB9vnrEee8zg4u9l99rA\"\nall_parts = [\"O5cR9\",\"0dXlH\",\"ZzEP8\"]\n\nemail = #### your code here ###"}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Task\n\nGiven, the explanation above, please fill in the following two constants in order to make the neural network work properly"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#### your code here ###\ndim = #### your code here ###\nsamples = #### your code here ###"}, {"cell_type": "markdown", "metadata": {}, "source": "### Submission\n\nNow it\u2019s time to submit your first solution. Please make sure that the secret variable contains a valid submission token. You can obtain it from the courser web page of the course using the grader section of this assignment.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "part = \"O5cR9\"\ntoken = #### your code here ### (have a look here if you need more information on how to obtain the token https://youtu.be/GcDo0Rwe06U?t=276)\n\nparts_data = {}\nparts_data[\"0dXlH\"] = json.dumps({\"number_of_neurons_layer1\": 0, \"number_of_neurons_layer2\": 0, \"number_of_neurons_layer3\": 0, \"number_of_epochs\": 0})\nparts_data[\"O5cR9\"] = json.dumps({\"dim\": dim, \"samples\": samples})\nparts_data[\"ZzEP8\"] = None \n\n\nsubmitAll(email, token, key, parts_data)"}, {"cell_type": "markdown", "metadata": {}, "source": "To observe how training works we just print the loss during training"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "class LossHistory(Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        sys.stdout.write(str(logs.get('loss'))+str(', '))\n        sys.stdout.flush()\n        self.losses.append(logs.get('loss'))\n        \nlr = LossHistory()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Task\n\nPlease fill in the following constants to properly configure the neural network. For some of them you have to find out the precise value, for others you can try and see how the neural network is performing at a later stage. The grader only looks at the values which need to be precise\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "number_of_neurons_layer1 = #### your code here ###\nnumber_of_neurons_layer2 = #### your code here ###\nnumber_of_neurons_layer3 = #### your code here ###\nnumber_of_epochs = #### your code here ###"}, {"cell_type": "markdown", "metadata": {}, "source": "###\u00a0Submission\n\nPlease submit your constants to the grader"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "parts_data = {}\nparts_data[\"0dXlH\"] = json.dumps({\"number_of_neurons_layer1\": number_of_neurons_layer1, \"number_of_neurons_layer2\": number_of_neurons_layer2, \"number_of_neurons_layer3\": number_of_neurons_layer3, \"number_of_epochs\": number_of_epochs})\nparts_data[\"O5cR9\"] = json.dumps({\"dim\": dim, \"samples\": samples})\nparts_data[\"ZzEP8\"] = None \n                                 \n                                 \ntoken = #### your code here ###\n\n\nsubmitAll(email, token, key, parts_data)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Task\n\nNow it\u2019s time to create the model. Please fill in the placeholders. Please note since this is only a toy example, we don't use a separate corpus for training and testing. Just use the same data for fitting and scoring\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# design network\nfrom tensorflow.keras import optimizers\nsgd = optimizers.SGD(lr=0.01, clipnorm=1.)\n\nmodel = Sequential()\nmodel.add(Dense(number_of_neurons_layer1,input_shape=(dim, ), activation='relu'))\nmodel.add(Dense(number_of_neurons_layer2, activation='relu'))\nmodel.add(Dense(number_of_neurons_layer3, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=sgd)\n\ndef train(data,label):\n    model.fit(#### your code here ###, #### your code here ###, epochs=number_of_epochs, batch_size=72, validation_data=(data, label), verbose=0, shuffle=True,callbacks=[lr])\n\ndef score(data):\n    return model.predict(data)"}, {"cell_type": "markdown", "metadata": {}, "source": "We prepare the training data by concatenating a label \u201c0\u201d for the broken and a label \u201c1\u201d for the healthy data. Finally we union the two data sets together"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "label_healthy = np.repeat(1,3)\nlabel_healthy.shape = (3,1)\nlabel_broken = np.repeat(0,3)\nlabel_broken.shape = (3,1)\n\ntrain_healthy = np.hstack((data_healthy_scaled,label_healthy))\ntrain_broken = np.hstack((data_broken_scaled,label_broken))\ntrain_both = np.vstack((train_healthy,train_broken))"}, {"cell_type": "markdown", "metadata": {}, "source": "Let\u2019s have a look at the two training sets for broken and healthy and at the union of them. Note that the last column is the label"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "pd.DataFrame(train_healthy)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "pd.DataFrame(train_broken)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "pd.DataFrame(train_both)"}, {"cell_type": "markdown", "metadata": {}, "source": "So those are frequency bands. Notice that although many frequency bands are having nearly the same energy, the neural network algorithm still can work those out which are significantly different. \n\n## Task\n\nNow it\u2019s time to do the training. Please provide the first 3000 columns of the array as the 1st parameter and column number 3000 containing the label as 2nd parameter. Please use the python array slicing syntax to obtain those. \n\nThe following link tells you more about the numpy array slicing syntax\nhttps://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "features = train_both[:,#### your code here ###]\nlabels = train_both[:,#### your code here ###]"}, {"cell_type": "markdown", "metadata": {}, "source": "Now it\u2019s time to do the training. You should see the loss trajectory go down, we will also plot it later. Note: We also could use TensorBoard for this but for this simple scenario we skip it. In some rare cases training doesn\u2019t converge simply because random initialization of the weights caused gradient descent to start at a sub-optimal spot on the cost hyperplane. Just recreate the model (the cell which contains *model = Sequential()*) and re-run all subsequent steps and train again\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train(features,labels)"}, {"cell_type": "markdown", "metadata": {}, "source": "Let's plot the losses"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(lr.losses)\nax.plot(range(0,size), lr.losses, '-', color='blue', animated = True, linewidth=1)"}, {"cell_type": "markdown", "metadata": {}, "source": "Now let\u2019s examine whether we are getting good results. Note: best practice is to use a training and a test data set for this which we\u2019ve omitted here for simplicity"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "score(data_healthy_scaled)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "score(data_broken_scaled)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Submission\n\nIn case you feel confident that everything works as it should (getting values close to one for the healthy and close to zero for the broken case) you can make sure that the secret variable contains a valid submission token and submit your work to the grader\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "parts_data = {}\nparts_data[\"0dXlH\"] = json.dumps({\"number_of_neurons_layer1\": number_of_neurons_layer1, \"number_of_neurons_layer2\": number_of_neurons_layer2, \"number_of_neurons_layer3\": number_of_neurons_layer3, \"number_of_epochs\": number_of_epochs})\nparts_data[\"O5cR9\"] = json.dumps({\"dim\": dim, \"samples\": samples})\n\n                                 \n                                 \ntoken = #### your code here ###"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "prediction = str(np.sum(score(data_healthy_scaled))/3)\nmyData={'healthy' : prediction}\nmyData\nparts_data[\"ZzEP8\"] = json.dumps(myData)\nsubmitAll(email, token, key, parts_data)"}], "metadata": {"kernelspec": {"display_name": "Python 3.6", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.9"}}, "nbformat": 4, "nbformat_minor": 1}